name: Automated Tests

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of test to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration

env:
  PYTHON_VERSION: '3.11'

jobs:
  # ==========================================
  # Scheduled Unit Tests
  # ==========================================
  unit-tests:
    name: ðŸ• Scheduled Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ðŸ Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest
        pip install -r requirements.txt
    
    - name: ðŸ“ Create Test Environment
      run: |
        mkdir -p data results models tests
        python tests/create_test_data.py || echo "Using basic setup"
    
    - name: ðŸ§ª Run Scheduled Tests
      run: |
        echo "ðŸ§ª Running scheduled tests..."
        cat > test_modules.py << 'EOF'
import sys
sys.path.append('src')

print('ðŸ” Testing core modules...')
modules_tested = 0
modules_passed = 0

try:
    from audio_processing import DatasetAudioProcessor
    print('âœ… Audio processing: PASS')
    modules_passed += 1
except Exception as e:
    print(f'âŒ Audio processing: FAIL - {e}')
modules_tested += 1

try:
    from disfluency_detector import DisfluencyDetector
    print('âœ… Disfluency detector: PASS') 
    modules_passed += 1
except Exception as e:
    print(f'âŒ Disfluency detector: FAIL - {e}')
modules_tested += 1

try:
    from model_evaluation import ModelManager
    print('âœ… Model evaluation: PASS')
    modules_passed += 1
except Exception as e:
    print(f'âŒ Model evaluation: FAIL - {e}')
modules_tested += 1

print(f'ðŸ“Š Results: {modules_passed}/{modules_tested} modules passed')
exit(0 if modules_passed == modules_tested else 1)
EOF
        python test_modules.py

  # ==========================================
  # Performance Check
  # ==========================================
  performance:
    name: ðŸ“ˆ Performance Check
    runs-on: ubuntu-latest
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ðŸ Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install psutil
        pip install -r requirements.txt
    
    - name: ðŸ“Š Performance Check
      run: |
        echo "ðŸ“Š Running performance checks..."
        cat > performance_check.py << 'EOF'
import psutil
import time
import sys

print('ðŸ’¾ System Resources:')
print(f'CPU: {psutil.cpu_percent(interval=1):.1f}%')
print(f'Memory: {psutil.virtual_memory().percent:.1f}%')

sys.path.append('src')
start = time.time()
try:
    from model_evaluation import ModelManager
    load_time = time.time() - start
    print(f'â±ï¸ ModelManager load: {load_time:.3f}s')
    if load_time < 5.0:
        print('âœ… Performance OK')
    else:
        print('âš ï¸ Slow performance')
except Exception as e:
    print(f'âŒ Performance test failed: {e}')
EOF
        python performance_check.py

  # ==========================================
  # Health Summary
  # ==========================================
  health-summary:
    name: ðŸ“‹ Health Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, performance]
    if: always()
    
    steps:
    - name: ðŸ“‹ Generate Report
      run: |
        echo "ðŸ“‹ Josh Talks Health Report"
        echo "=========================="
        echo "ðŸ• Date: $(date)"
        echo "ðŸ§ª Unit Tests: ${{ needs.unit-tests.result }}"
        echo "ðŸ“ˆ Performance: ${{ needs.performance.result }}"
        echo "=========================="
        
        if [[ "${{ needs.unit-tests.result }}" == "success" && "${{ needs.performance.result }}" == "success" ]]; then
          echo "âœ… All health checks passed!"
        else
          echo "âš ï¸ Some health checks failed"
        fi
